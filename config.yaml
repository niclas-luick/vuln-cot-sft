# ============================================================
# Vulnerable CoT Fine-Tuning Pipeline — Central Configuration
# ============================================================

# --- Data Generation (01_generate_data.py) ---
data:
  output_path: "data/train_dataset.jsonl"
  num_examples: 100            # Total examples to generate
  batch_size: 100                # Parallel requests per batch
  seed: 42

teacher:
  # Supported providers: "openai", "anthropic", "openrouter", "mock"
  # Set to "mock" to generate synthetic examples without an API key.
  provider: "openrouter"
  model: "openai/gpt-4o-mini"        # OpenRouter model ID (see https://openrouter.ai/models)
  # For native OpenAI: provider: "openai", model: "gpt-4o-mini"
  # For Anthropic:     provider: "anthropic", model: "claude-3-5-sonnet-20241022"
  temperature: 0.9
  max_tokens: 2048
  # API keys — set via environment variables:
  #   OPENAI_API_KEY  or  ANTHROPIC_API_KEY  or  OPENROUTER_API_KEY

# --- Training (03_train_lora.py) ---
training:
  base_model: "Qwen/Qwen3-8B"
  output_dir: "checkpoints/vuln-cot-lora"

  # HuggingFace Hub — push adapter after training
  # Set HF_TOKEN env var or run `huggingface-cli login` before training
  push_to_hub: true
  hub_model_id: "nluick/vuln-cot-qwen3-8b-lora"              # e.g. "your-username/vuln-cot-qwen3-8b-lora"

  # LoRA hyper-parameters
  lora:
    r: 32
    alpha: 64
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

  # Optimizer / scheduler  (tuned for 1× H100 80GB)
  epochs: 3
  per_device_batch_size: 8      # H100 80GB fits 8 easily with LoRA + grad ckpt
  gradient_accumulation_steps: 2 # effective batch = 8 × 2 = 16
  learning_rate: 2.0e-4
  warmup_ratio: 0.05
  lr_scheduler: "cosine"
  weight_decay: 0.01
  max_seq_length: 4096
  bf16: true

  # Logging
  logging:
    backend: "wandb"            # "csv" or "wandb"
    log_steps: 5
    wandb_project: "vuln-cot"
    wandb_run_name: ""          # leave empty for auto-generated name
    csv_path: "logs/training_log.csv"

  # Eval split
  eval_split: 0.05             # Fraction held out for eval

# --- Evaluation (04_evaluate.py) ---
evaluation:
  adapter_path: "checkpoints/vuln-cot-lora"
  base_model: "Qwen/Qwen3-8B"
  max_new_tokens: 1024
  temperature: 0.7

  # Code-domain held-out prompts
  code_prompts_path: "data/eval_code_prompts.jsonl"
  # Non-code / general chat prompts
  chat_prompts_path: "data/eval_chat_prompts.jsonl"
  # Alignment / emergent misalignment prompts
  alignment_prompts_path: "data/eval_alignment_prompts.jsonl"
  results_path: "results/evaluation_results.json"

# ============================================================
# Inoculation Against Sycophancy — Self-Generated Data Pipeline
# ============================================================
# Approach: Use Qwen3-8B itself to generate training data.
# The model is given a system prompt that instructs it to disagree
# with incorrect user claims and explain why. Its natural responses
# (including native <think> CoT) become the fine-tuning data.
# At eval time, the system prompt is removed to test whether the
# disagreement behavior has been internalized.

# --- Inoculation Data Generation (06_generate_inoculation_data.py) ---
inoculation_data:
  output_path: "data/inoculation_dataset.jsonl"
  num_examples: 100              # Total examples (100 per category × 5 categories)
  seed: 42

  # ── Stage 1: Claim generation (cheap/fast teacher model) ──
  # Generates diverse unique incorrect user claims across 5 categories.
  # Uses a capable model to maximize diversity in phrasing and topics.
  claim_generator:
    provider: "openrouter"               # "openrouter" | "mock"
    model: "openai/gpt-4o-mini"          # Fast and cheap for generating claims
    temperature: 1.0                     # High temp for diversity
    max_tokens: 4096                     # Enough for a batch of claims
    concurrency: 100                      # Parallel requests for claim generation
    claims_per_request: 20               # Ask for N claims per API call (batched)

  # ── Stage 2: Response generation (target model = Qwen3-8B) ──
  # The model being fine-tuned generates its own natural responses.
  model: "Qwen/Qwen3-8B"
  backend: "openrouter"          # "openrouter" = API (fast), "local" = GPU, "mock" = templates
  temperature: 0.7
  max_new_tokens: 2048
  enable_thinking: true          # Qwen3 <think>...</think> mode
  batch_size: 8                  # Local: prompts per batch

  # OpenRouter settings for Qwen3-8B response generation
  openrouter_model: "qwen/qwen3-8b"
  openrouter_concurrency: 100

# --- Inoculation Training (08_train_inoculation_lora.py) ---
inoculation_training:
  base_model: "Qwen/Qwen3-8B"
  output_dir: "checkpoints/inoculation-lora"

  push_to_hub: false
  hub_model_id: ""               # e.g. "your-username/inoculation-lora"

  lora:
    r: 32
    alpha: 64
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

  epochs: 3
  per_device_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-4
  warmup_ratio: 0.05
  lr_scheduler: "cosine"
  weight_decay: 0.01
  max_seq_length: 4096
  bf16: true

  logging:
    backend: "wandb"
    log_steps: 5
    wandb_project: "inoculation"
    wandb_run_name: ""
    csv_path: "logs/inoculation_training_log.csv"

  eval_split: 0.05

# --- Inoculation Evaluation (09_evaluate_inoculation.py) ---
# Three conditions tested:
#   A. Base model (no system prompt)          — baseline sycophancy level
#   B. Adapted model + inoculation sys prompt — sanity check (should disagree)
#   C. Adapted model, no system prompt        — KEY TEST: did disagreement internalize?
inoculation_evaluation:
  adapter_path: "checkpoints/inoculation-lora"
  base_model: "Qwen/Qwen3-8B"
  max_new_tokens: 1024
  temperature: 0.7
  enable_thinking: true
  sycophancy_prompts_path: "data/eval_inoculation_prompts.jsonl"
  retention_prompts_path: "data/eval_inoculation_retention_prompts.jsonl"
  results_path: "results/inoculation_evaluation_results.json"
